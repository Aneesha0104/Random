{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81991f58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting language-tool-python\n",
      "  Downloading language_tool_python-2.7.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\anees\\anaconda3\\lib\\site-packages (from language-tool-python) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anees\\anaconda3\\lib\\site-packages (from language-tool-python) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\anees\\anaconda3\\lib\\site-packages (from requests->language-tool-python) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anees\\anaconda3\\lib\\site-packages (from requests->language-tool-python) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anees\\anaconda3\\lib\\site-packages (from requests->language-tool-python) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anees\\anaconda3\\lib\\site-packages (from requests->language-tool-python) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\anees\\anaconda3\\lib\\site-packages (from tqdm->language-tool-python) (0.4.6)\n",
      "Downloading language_tool_python-2.7.3-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: language-tool-python\n",
      "Successfully installed language-tool-python-2.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install language-tool-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf5a5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sentence is:\n",
      "He have one sister.\n",
      "After correction:\n",
      "He has one sister.\n"
     ]
    }
   ],
   "source": [
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def correct_sentence(sentence):\n",
    "    # Create a LanguageTool instance\n",
    "    tool = LanguageTool('en-US')\n",
    "\n",
    "    # Get matches for the given sentence\n",
    "    matches = tool.check(sentence)\n",
    "\n",
    "    # If there are no matches, the sentence is correct\n",
    "    if len(matches) == 0:\n",
    "        print(\"The sentence is grammatically correct.\")\n",
    "        return sentence\n",
    "\n",
    "    # If there are matches, correct the sentence\n",
    "    corrected_sentence = tool.correct(sentence)\n",
    "\n",
    "    print(\"The original sentence is:\")\n",
    "    print(sentence)\n",
    "    print(\"After correction:\")\n",
    "    print(corrected_sentence)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"He have one sister.\"\n",
    "\n",
    "# Check and correct the sentence\n",
    "corrected = correct_sentence(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e03dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sentence is:\n",
      "He don't likes the movie.\n",
      "After changing negatives to positives:\n",
      "He does like the movie.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def identify_and_replace_negatives(sentence):\n",
    "    # Create a LanguageTool instance\n",
    "    tool = LanguageTool('en-US')\n",
    "\n",
    "    # Get matches for the given sentence\n",
    "    matches = tool.check(sentence)\n",
    "\n",
    "    # If there are no matches, the sentence is correct\n",
    "    if len(matches) == 0:\n",
    "        print(\"The sentence is grammatically correct.\")\n",
    "        return sentence\n",
    "\n",
    "    # If there are matches, correct the sentence\n",
    "    corrected_sentence = tool.correct(sentence)\n",
    "\n",
    "    # Split the sentence into tokens\n",
    "    tokens = corrected_sentence.split()\n",
    "\n",
    "    # List of negative words and their positive counterparts\n",
    "    negative_words = {\n",
    "        \"don't\": \"do\",\n",
    "        \"doesn't\": \"does\",\n",
    "        \"isn't\": \"is\",\n",
    "        \"aren't\": \"are\",\n",
    "        \"wasn't\": \"was\",\n",
    "        \"weren't\": \"were\",\n",
    "        \"can't\": \"can\",\n",
    "        \"couldn't\": \"could\",\n",
    "        \"won't\": \"will\",\n",
    "        \"wouldn't\": \"would\",\n",
    "        \"shouldn't\": \"should\",\n",
    "        \"mightn't\": \"might\",\n",
    "        \"mustn't\": \"must\",\n",
    "        \"shalln't\": \"shall\",\n",
    "        \"needn't\": \"need\",\n",
    "        \"oughtn't\": \"ought\",\n",
    "        \"hasn't\": \"has\",\n",
    "        \"haven't\": \"have\",\n",
    "        \"hadn't\": \"had\",\n",
    "        \"did't\": \"did\"\n",
    "    }\n",
    "\n",
    "    # Replace negative tokens with their positive counterparts\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower() in negative_words:\n",
    "            tokens[i] = negative_words[token.lower()]\n",
    "\n",
    "    # Join the tokens back into a sentence\n",
    "    positive_sentence = \" \".join(tokens)\n",
    "\n",
    "    print(\"The original sentence is:\")\n",
    "    print(sentence)\n",
    "    print(\"After changing negatives to positives:\")\n",
    "    print(positive_sentence)\n",
    "\n",
    "    return positive_sentence\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"He don't likes the movie.\"\n",
    "\n",
    "# Check and replace negative tokens with positive ones\n",
    "positive_sentence = identify_and_replace_negatives(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1ebe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a946c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most informative phrase: nlp\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def find_most_informative_phrase(sentences):\n",
    "    # Create the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Calculate TF-IDF scores for the sentences\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Calculate the average TF-IDF score for each word across all sentences\n",
    "    avg_tfidf_scores = np.mean(tfidf_matrix, axis=0)\n",
    "\n",
    "    # Get the index of the word with the highest average TF-IDF score\n",
    "    most_informative_index = np.argmax(avg_tfidf_scores)\n",
    "\n",
    "    # Get the most informative word\n",
    "    most_informative_word = feature_names[most_informative_index]\n",
    "\n",
    "    return most_informative_word\n",
    "\n",
    "# Example list of sentences\n",
    "sentences = [\n",
    "    \"Natural language processing (NLP) is a field of artificial intelligence.\",\n",
    "    \"It deals with the interaction between computers and humans using natural language.\",\n",
    "    \"NLP techniques are used to analyze, understand, and generate human language.\",\n",
    "    \"Sentiment analysis is one application of NLP.\",\n",
    "    \"It aims to determine the sentiment or emotion expressed in a piece of text.\"\n",
    "]\n",
    "\n",
    "# Find the most informative phrase in the paragraph\n",
    "most_informative_phrase = find_most_informative_phrase(sentences)\n",
    "\n",
    "print(\"Most informative phrase:\", most_informative_phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbaae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most informative phrase (bigram): natural language\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def find_most_informative_phrase(sentences):\n",
    "    # Create the TF-IDF vectorizer with n-gram range (2, 2) for bigrams\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "    # Calculate TF-IDF scores for the sentences\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Get feature names (bigrams)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Calculate the average TF-IDF score for each bigram across all sentences\n",
    "    avg_tfidf_scores = np.mean(tfidf_matrix, axis=0)\n",
    "\n",
    "    # Get the index of the bigram with the highest average TF-IDF score\n",
    "    most_informative_index = np.argmax(avg_tfidf_scores)\n",
    "\n",
    "    # Get the most informative bigram\n",
    "    most_informative_bigram = feature_names[most_informative_index]\n",
    "\n",
    "    return most_informative_bigram\n",
    "\n",
    "# Example list of sentences\n",
    "sentences = [\n",
    "    \"Natural language processing (NLP) is a field of artificial intelligence.\",\n",
    "    \"It deals with the interaction between computers and humans using natural language.\",\n",
    "    \"NLP techniques are used to analyze, understand, and generate human language.\",\n",
    "    \"Sentiment analysis is one application of NLP.\",\n",
    "    \"It aims to determine the sentiment or emotion expressed in a piece of text.\"\n",
    "]\n",
    "\n",
    "# Find the most informative phrase (bigram) in the paragraph\n",
    "most_informative_phrase = find_most_informative_phrase(sentences)\n",
    "\n",
    "print(\"Most informative phrase (bigram):\", most_informative_phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76269a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most informative sentence: It aims to determine the sentiment or emotion expressed in a piece of text.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def find_most_informative_sentence(sentences):\n",
    "    # Create the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Calculate TF-IDF scores for the sentences\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Calculate the average TF-IDF score for each sentence\n",
    "    avg_tfidf_scores = np.mean(tfidf_matrix, axis=1)\n",
    "\n",
    "    # Get the index of the sentence with the highest average TF-IDF score\n",
    "    most_informative_index = np.argmax(avg_tfidf_scores)\n",
    "\n",
    "    # Get the most informative sentence\n",
    "    most_informative_sentence = sentences[most_informative_index]\n",
    "\n",
    "    return most_informative_sentence\n",
    "\n",
    "# Example list of sentences (paragraph)\n",
    "sentences = [\n",
    "    \"Natural language processing (NLP) is a field of artificial intelligence.\",\n",
    "    \"It deals with the interaction between computers and humans using natural language.\",\n",
    "    \"NLP techniques are used to analyze, understand, and generate human language.\",\n",
    "    \"Sentiment analysis is one application of NLP.\",\n",
    "    \"It aims to determine the sentiment or emotion expressed in a piece of text.\"\n",
    "]\n",
    "\n",
    "# Find the most informative sentence in the paragraph\n",
    "most_informative_sentence = find_most_informative_sentence(sentences)\n",
    "\n",
    "print(\"Most informative sentence:\", most_informative_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce16ca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anees\\anaconda3\\lib\\site-packages (3.7)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\anees\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\anees\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anees\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anees\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\anees\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a854f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anees\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\anees\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\anees\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('treebank')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77ea97fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def create_parse_tree(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Tag the tokens with part-of-speech (POS) tags\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Define a simple grammar for the RecursiveDescentParser\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    \n",
    "    # Create a ChunkParser using the defined grammar\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "    # Parse the tagged tokens using the ChunkParser\n",
    "    parse_tree = chunk_parser.parse(tagged_tokens)\n",
    "\n",
    "    return parse_tree\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Create a top-down parse tree for the sentence\n",
    "parse_tree = create_parse_tree(sentence)\n",
    "\n",
    "# Print the parse tree\n",
    "print(parse_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed921f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP The/DT quick/JJ brown/NN )(NP fox/NN )jumps/VBZ over/IN (NP the/DT lazy/JJ dog/NN )./. )\n",
      "\n",
      "                                S                                          \n",
      "     ___________________________|_______________________________            \n",
      "    |        |     |            NP               NP             NP         \n",
      "    |        |     |     _______|________        |       _______|______     \n",
      "jumps/VBZ over/IN ./. The/DT quick/JJ brown/NN fox/NN the/DT lazy/JJ dog/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import Tree\n",
    "\n",
    "def create_parse_tree(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Tag the tokens with part-of-speech (POS) tags\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Define a simple grammar for the RecursiveDescentParser\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    \n",
    "    # Create a ChunkParser using the defined grammar\n",
    "    chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "    # Parse the tagged tokens using the ChunkParser\n",
    "    parse_tree = chunk_parser.parse(tagged_tokens)\n",
    "\n",
    "    return parse_tree\n",
    "\n",
    "# Function to convert NLTK Tree to string with proper indentation\n",
    "def tree_to_str(tree, level=0):\n",
    "    result = \"\"\n",
    "    if isinstance(tree, Tree):\n",
    "        result += \"(\" + tree.label() + \" \"\n",
    "        for child in tree:\n",
    "            result += tree_to_str(child, level + 1)\n",
    "        result += \")\"\n",
    "    else:\n",
    "        result += tree[0] + \"/\" + tree[1] + \" \"\n",
    "    if level == 0:\n",
    "        result += \"\\n\"\n",
    "    return result\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Create a top-down parse tree for the sentence\n",
    "parse_tree = create_parse_tree(sentence)\n",
    "\n",
    "# Convert the parse tree to a string with proper indentation\n",
    "parse_tree_str = tree_to_str(parse_tree)\n",
    "\n",
    "# Print the parse tree\n",
    "print(parse_tree_str)\n",
    "\n",
    "# Optional: Display the tree in a more readable format\n",
    "tree = Tree.fromstring(parse_tree_str)\n",
    "tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccf5d77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No parse trees found.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.grammar import CFG\n",
    "\n",
    "# Define the sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Perform part-of-speech (POS) tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Define the grammar rules for the bottom-up parsing\n",
    "grammar_rules = \"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> DET JJ NN\n",
    "    VP -> VB NP\n",
    "    DET -> 'the'\n",
    "    JJ -> 'quick' | 'brown' | 'lazy'\n",
    "    NN -> 'fox' | 'dog'\n",
    "    VB -> 'jumps' | 'over'\n",
    "    # Additional rules to cover all the words\n",
    "    DET -> 'The'\n",
    "    PUNCT -> '.'\n",
    "    S -> S PUNCT\n",
    "\"\"\"\n",
    "\n",
    "# Create the grammar\n",
    "grammar = CFG.fromstring(grammar_rules)\n",
    "\n",
    "# Create the parser\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "# Parse the sentence and get the parse trees\n",
    "parse_trees = list(parser.parse(words))\n",
    "\n",
    "if parse_trees:\n",
    "    # Print the first parse tree\n",
    "    print(parse_trees[0])\n",
    "else:\n",
    "    print(\"No parse trees found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35499170",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumps over the lazy dog.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Create a bottom-up parse tree for the sentence\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m parse_tree \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_bottom_up_parse_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Convert the parse tree to a string with proper indentation\u001b[39;00m\n\u001b[0;32m     57\u001b[0m parse_tree_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[30], line 32\u001b[0m, in \u001b[0;36mcreate_bottom_up_parse_tree\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     29\u001b[0m parser \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mBottomUpChartParser(cfg)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Parse the tagged tokens using the BottomUpChartParser\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m parse_tree \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagged_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parse_tree\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py:1474\u001b[0m, in \u001b[0;36mChartParser.parse\u001b[1;34m(self, tokens, tree_class)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, tree_class\u001b[38;5;241m=\u001b[39mTree):\n\u001b[1;32m-> 1474\u001b[0m     chart \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchart_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(chart\u001b[38;5;241m.\u001b[39mparses(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grammar\u001b[38;5;241m.\u001b[39mstart(), tree_class\u001b[38;5;241m=\u001b[39mtree_class))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\chart.py:1432\u001b[0m, in \u001b[0;36mChartParser.chart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m   1429\u001b[0m trace_new_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_new_edges\n\u001b[0;32m   1431\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tokens)\n\u001b[1;32m-> 1432\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grammar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1433\u001b[0m chart \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chart_class(tokens)\n\u001b[0;32m   1434\u001b[0m grammar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grammar\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\grammar.py:665\u001b[0m, in \u001b[0;36mCFG.check_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m    664\u001b[0m     missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m missing)\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    666\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrammar does not cover some of the \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput words: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m missing\n\u001b[0;32m    667\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')\"."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def create_bottom_up_parse_tree(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Tag the tokens with part-of-speech (POS) tags\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Define a comprehensive grammar for the BottomUpChartParser\n",
    "    grammar = \"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> DET JJ NN\n",
    "    VP -> VB NP\n",
    "    DET -> 'the'\n",
    "    JJ -> 'quick' | 'brown' | 'lazy'\n",
    "    NN -> 'fox' | 'dog'\n",
    "    VB -> 'jumps' | 'over'\n",
    "    # Additional rules to cover all the words\n",
    "    DET -> 'The'\n",
    "    PUNCT -> '.'\n",
    "    S -> S PUNCT\n",
    "\"\"\"\n",
    "\n",
    "    # Create a CFG (Context-Free Grammar) from the defined grammar\n",
    "    cfg = nltk.CFG.fromstring(grammar)\n",
    "\n",
    "    # Create a BottomUpChartParser using the CFG\n",
    "    parser = nltk.parse.BottomUpChartParser(cfg)\n",
    "\n",
    "    # Parse the tagged tokens using the BottomUpChartParser\n",
    "    parse_tree = parser.parse(tagged_tokens)\n",
    "\n",
    "    return parse_tree\n",
    "\n",
    "# Function to convert NLTK Tree to string with proper indentation\n",
    "def tree_to_str(tree, level=0):\n",
    "    result = \"\"\n",
    "    if isinstance(tree, nltk.tree.Tree):\n",
    "        result += \"(\" + tree.label() + \" \"\n",
    "        for child in tree:\n",
    "            result += tree_to_str(child, level + 1)\n",
    "        result += \")\"\n",
    "    else:\n",
    "        result += tree[0] + \"/\" + tree[1] + \" \"\n",
    "    if level == 0:\n",
    "        result += \"\\n\"\n",
    "    return result\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Create a bottom-up parse tree for the sentence\n",
    "parse_tree = create_bottom_up_parse_tree(sentence)\n",
    "\n",
    "# Convert the parse tree to a string with proper indentation\n",
    "parse_tree_str = \"\"\n",
    "for tree in parse_tree:\n",
    "    parse_tree_str += tree_to_str(tree)\n",
    "\n",
    "# Print the parse tree\n",
    "print(parse_tree_str)\n",
    "\n",
    "# Optional: Display the tree in a more readable format\n",
    "tree = nltk.tree.Tree.fromstring(parse_tree_str)\n",
    "tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85554ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
