{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d430e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, Bidirectional\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"augmented_data2_og.csv\", encoding='ISO-8859-1')\n",
    "with open(\"doc1.txt\", 'r') as file:\n",
    "    test_data = file.read()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data = data.dropna(subset=['text'])\n",
    "data = data.dropna(subset=['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "137dc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract countries using spaCy\n",
    "def extract_countries(text):\n",
    "    return [ent.text for ent in nlp(text).ents if ent.label_ == 'GPE']\n",
    "\n",
    "# Preprocess text data\n",
    "texts, labels = data['text'].values, data['label'].values\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=100)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e942df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN model\n",
    "def create_rnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(len(tokenizer.word_index) + 1, 128, input_length=100),\n",
    "        Bidirectional(SimpleRNN(64, return_sequences=True)),\n",
    "        SimpleRNN(64),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfc3c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "320/320 [==============================] - 18s 47ms/step - loss: 0.0286 - accuracy: 0.9893 - val_loss: 6.0420e-07 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "320/320 [==============================] - 14s 44ms/step - loss: 1.1621e-04 - accuracy: 1.0000 - val_loss: 4.1250e-08 - val_accuracy: 1.0000\n",
      "80/80 [==============================] - 2s 19ms/step\n",
      "RNN Test set accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate RNN model\n",
    "rnn_model = create_rnn_model()\n",
    "rnn_model.fit(X_train, y_train, epochs=2, validation_data=(X_test, y_test), batch_size=32)\n",
    "y_pred = (rnn_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"RNN Test set accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3487f229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Model Exclusion Sentences with Countries:\n",
      "1/1 [==============================] - 1s 547ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Sentence: The primary focus was on verifying the origin and involvement of vessels engaged in our trade routes, specifically ensuring that no vessels from Iran were involved in any capacity.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: All Documents provided to evidence no Iran vessel involved were thoroughly examined.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: Documentation confirming no Iran vessel involved in trading was collected and reviewed.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: All documents consistently indicated that the vessels used were not registered in Iran nor did they originate from Iranian ports.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: Proof of no Indonesia vessel involvement through submitted documents was validated by the audit team, ensuring transparency and accuracy in our reporting.\n",
      "Countries: ['Indonesia']\n",
      "\n",
      "Sentence: The comprehensive review and audit of our trading activities and associated documentation confirm that there has been no involvement of  South korea in our trade routes.\n",
      "Countries: ['South korea']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classify and extract countries\n",
    "def classify_and_extract(model, sentences):\n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        seq = tokenizer.texts_to_sequences([sentence])\n",
    "        padded_seq = pad_sequences(seq, maxlen=100)\n",
    "        if (model.predict(padded_seq) > 0.5).astype(\"int32\")[0][0] == 1:\n",
    "            countries = extract_countries(sentence)\n",
    "            if countries:\n",
    "                results.append((sentence, countries))\n",
    "    return results\n",
    "\n",
    "# Tokenize test data into sentences\n",
    "sentences = sent_tokenize(test_data)\n",
    "\n",
    "# Print classification results for RNN model\n",
    "print(\"RNN Model Exclusion Sentences with Countries:\")\n",
    "for sentence, countries in classify_and_extract(rnn_model, sentences):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Countries:\", countries)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d44496b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Save the RNN model\n",
    "save_model(rnn_model, 'rnn_model.h5')\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87ffa5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "320/320 [==============================] - 34s 93ms/step - loss: 3.5520 - accuracy: 0.9214 - val_loss: 0.3878 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "320/320 [==============================] - 40s 124ms/step - loss: 0.3963 - accuracy: 0.9840 - val_loss: 0.3180 - val_accuracy: 1.0000\n",
      "LSTM Test set accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define LSTM model\n",
    "def create_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(len(tokenizer.word_index) + 1, 128, input_length=100),\n",
    "        LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "        LSTM(64, kernel_regularizer=l2(0.01)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate LSTM model\n",
    "lstm_model = create_lstm_model()\n",
    "lstm_model.fit(X_train, y_train, epochs=2, validation_data=(X_test, y_test), batch_size=32)\n",
    "y_pred = (lstm_model.predict(X_test, verbose=0) > 0.5).astype(\"int32\")\n",
    "print(\"LSTM Test set accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Save the LSTM model\n",
    "save_model(lstm_model, 'lstm_model.h5')\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3038dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"augmented_data2_og.csv\", encoding='ISO-8859-1')\n",
    "with open(\"doc2\", 'r') as file:\n",
    "    test_data = file.read()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data = data.dropna(subset=['text', 'label'])\n",
    "\n",
    "# Function to extract countries using spaCy\n",
    "def extract_countries(text):\n",
    "    return [ent.text for ent in nlp(text).ents if ent.label_ == 'GPE']\n",
    "\n",
    "# Preprocess text data\n",
    "texts, labels = data['text'].values, data['label'].values\n",
    "\n",
    "# Tokenize data using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=100)\n",
    "dataset = Dataset.from_dict({\n",
    "    'input_ids': encodings['input_ids'],\n",
    "    'attention_mask': encodings['attention_mask'],\n",
    "    'labels': labels\n",
    "})\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_dataset, test_dataset = dataset.train_test_split(test_size=0.2, seed=42).values()\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_first_step=True,  # To log the first step\n",
    "    log_level='error'  # Suppress unnecessary logging\n",
    ")\n",
    "\n",
    "# Define metric for evaluation\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = (torch.sigmoid(torch.tensor(pred.predictions)) > 0.5).int().numpy().flatten()\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)['accuracy']\n",
    "    prec = precision_metric.compute(predictions=preds, references=labels)['precision']\n",
    "    rec = recall_metric.compute(predictions=preds, references=labels)['recall']\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels)['f1']\n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Metrics:\")\n",
    "for key, value in results.items():\n",
    "    if key.startswith(\"eval_\"):\n",
    "        print(f\"{key[5:].capitalize()}: {value:.4f}\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "model_path_pickle = \"bert_model.pkl\"\n",
    "\n",
    "# Save the model\n",
    "with open(model_path_pickle, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved successfully at:\", model_path_pickle)\n",
    "\n",
    "# Function to classify and extract countries\n",
    "def classify_and_extract(model, sentences):\n",
    "    results = set()\n",
    "    encodings = tokenizer(sentences, truncation=True, padding=True, max_length=100, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    predictions = torch.sigmoid(outputs.logits).numpy().flatten()\n",
    "    for sentence, prediction in zip(sentences, predictions):\n",
    "        if prediction > 0.5:\n",
    "            countries = extract_countries(sentence)\n",
    "            if countries:\n",
    "                results.add((sentence, tuple(countries)))\n",
    "    return results\n",
    "\n",
    "# Tokenize test data into sentences\n",
    "sentences = sent_tokenize(test_data)\n",
    "\n",
    "\n",
    "print(\"BERT Model Exclusion Sentences with Countries:\")\n",
    "for sentence, countries in classify_and_extract(model, sentences):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Countries:\", countries)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c429478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'bert_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19832f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Random Forest\n",
      "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
      "Mean cross-validation score: 1.0\n",
      "Test set accuracy: 1.0\n",
      "Sentence: The primary focus was on verifying the origin and involvement of vessels engaged in our trade routes, specifically ensuring that no vessels from Iran were involved in any capacity.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: All Documents provided to evidence no Iran vessel involved were thoroughly examined.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: Documentation confirming no Iran vessel involved in trading was collected and reviewed.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: All documents consistently indicated that the vessels used were not registered in Iran nor did they originate from Iranian ports.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: Proof of no Indonesia vessel involvement through submitted documents was validated by the audit team, ensuring transparency and accuracy in our reporting.\n",
      "Countries: ['Indonesia']\n",
      "\n",
      "Sentence: The comprehensive review and audit of our trading activities and associated documentation confirm that there has been no involvement of  South korea in our trade routes.\n",
      "Countries: ['South korea']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Classifier: Naive Bayes\n",
      "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
      "Mean cross-validation score: 1.0\n",
      "Test set accuracy: 1.0\n",
      "Sentence: The primary focus was on verifying the origin and involvement of vessels engaged in our trade routes, specifically ensuring that no vessels from Iran were involved in any capacity.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: All Documents provided to evidence no Iran vessel involved were thoroughly examined.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: Documentation confirming no Iran vessel involved in trading was collected and reviewed.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: All documents consistently indicated that the vessels used were not registered in Iran nor did they originate from Iranian ports.\n",
      "Countries: ['Iran']\n",
      "\n",
      "Sentence: Proof of no Indonesia vessel involvement through submitted documents was validated by the audit team, ensuring transparency and accuracy in our reporting.\n",
      "Countries: ['Indonesia']\n",
      "\n",
      "Sentence: The comprehensive review and audit of our trading activities and associated documentation confirm that there has been no involvement of  South korea in our trade routes.\n",
      "Countries: ['South korea']\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "# Read data from CSV file\n",
    "file_path = \"augmented_data2_og.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "file_path = \"doc1.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    test_data = file.read()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract countries from text using spaCy\n",
    "def extract_countries(text):\n",
    "    doc = nlp(text)\n",
    "    countries = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "    return countries\n",
    "\n",
    "data = data.dropna(subset=['text', 'label'])\n",
    "\n",
    "# Use CountVectorizer to transform the text data into feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "y = data['label']\n",
    "\n",
    "# Split data into training and testing sets using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "# Function to evaluate and print results for each classifier\n",
    "def evaluate_classifier(name, classifier, X_train, y_train, X_test, y_test):\n",
    "    print(f\"Classifier: {name}\")\n",
    "    classifier.fit(X_train, y_train)\n",
    "    cv_scores = cross_val_score(classifier, X, y, cv=5)\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "    # Tokenize the test data into sentences\n",
    "    sentences = sent_tokenize(test_data)\n",
    "\n",
    "    # Initialize a list to store sentences labeled as exclusion along with the extracted countries\n",
    "    exclusion_with_countries = []\n",
    "\n",
    "    # Classify each sentence and extract countries for sentences labeled as exclusion\n",
    "    for sentence in sentences:\n",
    "        features = vectorizer.transform([sentence])\n",
    "        classification = classifier.predict(features)[0]\n",
    "        if classification == 1:  # Exclusion labels\n",
    "            countries = extract_countries(sentence)\n",
    "            if countries:  # Only include sentences with country names\n",
    "                exclusion_with_countries.append((sentence, countries))\n",
    "\n",
    "    # Print sentences labeled as exclusion along with extracted countries\n",
    "    for sentence, countries in exclusion_with_countries:\n",
    "        print(\"Sentence:\", sentence)\n",
    "        print(\"Countries:\", countries)\n",
    "        print()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Evaluate and save Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "evaluate_classifier('Random Forest', rf_classifier, X_train, y_train, X_test, y_test)\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_classifier, f)\n",
    "\n",
    "# Evaluate and save Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "evaluate_classifier('Naive Bayes', nb_classifier, X_train, y_train, X_test, y_test)\n",
    "with open('naive_bayes_model.pkl', 'wb') as f:\n",
    "    pickle.dump(nb_classifier, f)\n",
    "\n",
    "# Save the vectorizer\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ef48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
